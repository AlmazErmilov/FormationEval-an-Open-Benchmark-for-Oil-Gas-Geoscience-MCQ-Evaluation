% References for FormationEval paper

% =============================================================================
% Related Work: AI/ML in geoscience
% =============================================================================

@article{hadid_genai_geoscience,
  title = {When Geoscience Meets Generative {AI} and Large Language Models: Foundations, Trends, and Future Challenges},
  author = {Hadid, Abdenour and Chakraborty, Tanujit and Busby, Daniel},
  journal = {Expert Systems},
  volume = {41},
  number = {10},
  year = {2024},
  doi = {10.1111/exsy.13654},
  url = {https://arxiv.org/abs/2402.03349}
}

@inproceedings{k2_geoscience,
  title = {{K2}: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization},
  author = {Deng, Cheng and Zhang, Tianhang and He, Zhongmou and Xu, Yi and Chen, Qiyuan and Shi, Yuzhong and Fu, Luoyi and Zhang, Weinan and Wang, Xinbing and Zhou, Chenghu and Lin, Zhouhan and He, Junxian},
  booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining (WSDM)},
  year = {2024},
  doi = {10.1145/3616855.3635772},
  url = {https://arxiv.org/abs/2306.05064}
}

@article{geogalactica,
  title = {{GeoGalactica}: A Scientific Large Language Model in Geoscience},
  author = {Lin, Zhouhan and Deng, Cheng and Zhou, Le and Zhang, Tianhang and Xu, Yi and Xu, Yutong and He, Zhongmou and Shi, Yuzhong and Dai, Beiya and Song, Yunchong and others},
  journal = {arXiv preprint arXiv:2401.00434},
  year = {2024},
  url = {https://arxiv.org/abs/2401.00434}
}

% =============================================================================
% Related Work: domain-specific benchmarks
% =============================================================================

@article{mmlu_pro,
  title = {{MMLU-Pro}: A More Robust and Challenging Multi-Task Language Understanding Benchmark},
  author = {Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and others},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2024},
  url = {https://arxiv.org/abs/2406.01574}
}

@article{gpqa,
  title = {{GPQA}: A Graduate-Level {Google-Proof} {Q\&A} Benchmark},
  author = {Rein, David and Hou, Betty Li and others},
  journal = {arXiv preprint arXiv:2311.12022},
  year = {2023},
  url = {https://arxiv.org/abs/2311.12022}
}

@article{arc,
  title = {Think You Have Solved Question Answering? Try {ARC}, the {AI2} Reasoning Challenge},
  author = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal = {arXiv preprint arXiv:1803.05457},
  year = {2018},
  url = {https://arxiv.org/abs/1803.05457}
}

@article{scibench,
  title = {{SciBench}: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models},
  author = {Wang, Xiaoxuan and Hu, Ziniu and Lu, Pan and Zhu, Yanqiao and Zhang, Jieyu and Subramaniam, Satyen and Loomba, Arjun R. and Zhang, Shicheng and Sun, Yixin and Wang, Wei},
  journal = {arXiv preprint arXiv:2307.10635},
  year = {2023},
  url = {https://arxiv.org/abs/2307.10635}
}

% =============================================================================
% Related Work: MCQ generation methods
% =============================================================================

@article{mcq_survey,
  title = {Automatic Multiple Choice Question Generation from Text: A Survey},
  author = {Ch, Dhawaleswar Rao and Saha, Sujan Kumar},
  journal = {IEEE Transactions on Learning Technologies},
  volume = {13},
  number = {1},
  pages = {14--25},
  year = {2020},
  doi = {10.1109/TLT.2018.2889100}
}

@article{distractor_survey,
  title = {Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation},
  author = {Alhazmi, Elaf and Sheng, Quan Z. and Zhang, Wei Emma and Zaib, Munazza and Alhazmi, Ahoud},
  journal = {Proceedings of EMNLP},
  year = {2024},
  url = {https://arxiv.org/abs/2402.01512}
}

@article{mcqg_srefine,
  title = {{MCQG-SRefine}: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback},
  author = {Yao, Zonghai and Parashar, Aditya and Zhou, Huixue and Jang, Won Seok and Ouyang, Fei and Yang, Zhichao and Yu, Hong},
  journal = {Proceedings of NAACL},
  year = {2025},
  url = {https://arxiv.org/abs/2410.13191}
}

% =============================================================================
% Domain-specific benchmarks (for Related Work section)
% =============================================================================
@article{medqa,
  title = {What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},
  author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  journal = {Applied Sciences},
  volume = {11},
  number = {14},
  pages = {6421},
  year = {2021},
  doi = {10.3390/app11146421},
  url = {https://arxiv.org/abs/2009.13081}
}

@inproceedings{legalbench,
  title = {{LegalBench}: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},
  author = {Guha, Neel and Nyarko, Julian and Ho, Daniel E. and R{\'e}, Christopher and Chilton, Adam and others},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2023},
  url = {https://arxiv.org/abs/2308.11462}
}

@article{mmlu,
  title = {Measuring Massive Multitask Language Understanding},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal = {arXiv preprint arXiv:2009.03300},
  year = {2021},
  url = {https://arxiv.org/abs/2009.03300}
}

@book{ellis_singer_2007,
  title = {Well Logging for Earth Scientists},
  author = {Ellis, Darwin V. and Singer, Julian M.},
  year = {2007},
  edition = {2nd},
  publisher = {Springer},
  address = {Dordrecht},
  isbn = {978-1-4020-4602-5},
  doi = {10.1007/978-1-4020-4602-5},
  url = {https://doi.org/10.1007/978-1-4020-4602-5}
}

@book{bjorlykke_2010,
  title = {Petroleum Geoscience: From Sedimentary Environments to Rock Physics},
  author = {Bj{\o}rlykke, Knut},
  year = {2010},
  publisher = {Springer},
  address = {Berlin},
  isbn = {978-3-642-02332-3},
  doi = {10.1007/978-3-642-02332-3},
  url = {https://doi.org/10.1007/978-3-642-02332-3}
}

@misc{tudelft_ocw_2008,
  title = {Petroleum Geology},
  author = {{TU Delft OpenCourseWare}},
  year = {2008},
  howpublished = {\url{https://ocw.tudelft.nl/courses/petroleum-geology/}},
  note = {Accessed December 2025}
}

@misc{formationeval_repo,
  title = {{FormationEval}: An Open Benchmark for Oil and Gas Geoscience {MCQ} Evaluation},
  author = {Ermilov, Almaz},
  year = {2025},
  howpublished = {\url{https://github.com/AlmazErmilov/FormationEval-an-Open-Benchmark-for-Oil-Gas-Geoscience-MCQ-Evaluation}},
  note = {Repository with benchmark data and evaluation code. Accessed December 2025}
}

@misc{formationeval_dataset,
  title = {{FormationEval} v0.1 Dataset},
  author = {Ermilov, Almaz},
  year = {2025},
  howpublished = {\url{https://github.com/AlmazErmilov/FormationEval-an-Open-Benchmark-for-Oil-Gas-Geoscience-MCQ-Evaluation/blob/v0.1/data/benchmark/formationeval_v0.1.json}},
  note = {505 MCQs in JSON format with source metadata. Accessed December 2025}
}

@misc{formationeval_pdf,
  title = {{FormationEval} v0.1 Benchmark PDF},
  author = {Ermilov, Almaz},
  year = {2025},
  howpublished = {\url{https://github.com/AlmazErmilov/FormationEval-an-Open-Benchmark-for-Oil-Gas-Geoscience-MCQ-Evaluation/blob/v0.1/data/benchmark/formationeval_v0.1.pdf}},
  note = {PDF export of the benchmark for reading and review. Accessed December 2025}
}

@misc{formationeval_leaderboard,
  title = {{FormationEval} v0.1 Leaderboard},
  author = {Ermilov, Almaz},
  year = {2025},
  howpublished = {\url{https://github.com/AlmazErmilov/FormationEval-an-Open-Benchmark-for-Oil-Gas-Geoscience-MCQ-Evaluation/blob/v0.1/eval/results/leaderboard.md}},
  note = {Evaluation leaderboard results. Accessed December 2025}
}

@misc{formationeval_analysis,
  title = {{FormationEval} v0.1 Analysis},
  author = {Ermilov, Almaz},
  year = {2025},
  howpublished = {\url{https://github.com/AlmazErmilov/FormationEval-an-Open-Benchmark-for-Oil-Gas-Geoscience-MCQ-Evaluation/blob/v0.1/eval/results/analysis.md}},
  note = {Hardest questions and bias analysis. Accessed December 2025}
}


% =============================================================================
% API Providers
% =============================================================================

@misc{azure_openai,
  title = {{Azure OpenAI Service} Documentation},
  author = {{Microsoft}},
  year = {2025},
  howpublished = {\url{https://learn.microsoft.com/en-us/azure/ai-services/openai/}},
  note = {Accessed December 2025}
}

@misc{openrouter,
  title = {{OpenRouter}: Unified API for {AI} Models},
  author = {{OpenRouter}},
  year = {2025},
  howpublished = {\url{https://openrouter.ai/docs}},
  note = {Accessed December 2025}
}

% =============================================================================
% Model Technical Reports and Model Cards
% =============================================================================

% Alibaba - Qwen
@article{qwen3,
  title = {Qwen3 Technical Report},
  author = {{Qwen Team}},
  journal = {arXiv preprint arXiv:2505.09388},
  year = {2025},
  url = {https://arxiv.org/abs/2505.09388}
}

% Anthropic - Claude
@misc{claude3,
  title = {The {Claude} 3 Model Family: Opus, Sonnet, Haiku},
  author = {{Anthropic}},
  year = {2024},
  howpublished = {\url{https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf}},
  note = {Model card. Accessed December 2025}
}

@misc{claude35,
  title = {Model Card Addendum: {Claude} 3.5 Haiku and Upgraded {Claude} 3.5 Sonnet},
  author = {{Anthropic}},
  year = {2024},
  howpublished = {\url{https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf}},
  note = {Model card addendum. Accessed December 2025}
}

@misc{claude37,
  title = {{Claude} 3.7 Sonnet System Card},
  author = {{Anthropic}},
  year = {2025},
  howpublished = {\url{https://www.anthropic.com/claude-3-7-sonnet-system-card}},
  note = {System card. Accessed December 2025}
}

@misc{claude_opus45,
  title = {{Claude} Opus 4.5 System Card},
  author = {{Anthropic}},
  year = {2025},
  howpublished = {\url{https://www.anthropic.com/claude-opus-4-5-system-card}},
  note = {System card. Accessed December 2025}
}

% DeepSeek
@article{deepseek_v3,
  title = {{DeepSeek-V3} Technical Report},
  author = {{DeepSeek-AI}},
  journal = {arXiv preprint arXiv:2412.19437},
  year = {2024},
  url = {https://arxiv.org/abs/2412.19437}
}

@misc{deepseek_v3_2,
  title = {{DeepSeek-V3.2} Model Card},
  author = {{DeepSeek-AI}},
  year = {2025},
  howpublished = {\url{https://huggingface.co/deepseek-ai/DeepSeek-V3.2}},
  note = {Hugging Face model card. Accessed December 2025}
}

@article{deepseek_r1,
  title = {{DeepSeek-R1}: Incentivizing Reasoning Capability in {LLMs} via Reinforcement Learning},
  author = {{DeepSeek-AI}},
  journal = {arXiv preprint arXiv:2501.12948},
  year = {2025},
  url = {https://arxiv.org/abs/2501.12948}
}

% Google - Gemini
@article{gemini,
  title = {Gemini: A Family of Highly Capable Multimodal Models},
  author = {{Gemini Team, Google}},
  journal = {arXiv preprint arXiv:2312.11805},
  year = {2023},
  url = {https://arxiv.org/abs/2312.11805}
}

@misc{gemini2,
  title = {Gemini 2 Flash Model Card},
  author = {{Google DeepMind}},
  year = {2025},
  howpublished = {\url{https://modelcards.withgoogle.com/assets/documents/gemini-2-flash.pdf}},
  note = {Model card. Accessed December 2025}
}

@article{gemini25,
  title = {Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities},
  author = {{Gemini Team, Google DeepMind}},
  journal = {arXiv preprint arXiv:2507.06261},
  year = {2025},
  url = {https://arxiv.org/abs/2507.06261}
}

@misc{gemini3,
  title = {Gemini 3 Pro Model Card},
  author = {{Google DeepMind}},
  year = {2025},
  howpublished = {\url{https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf}},
  note = {Model card. Accessed December 2025}
}

% Google - Gemma
@misc{gemma3,
  title = {Gemma 3 Model Card},
  author = {{Google DeepMind}},
  year = {2025},
  howpublished = {\url{https://ai.google.dev/gemma/docs/core/model_card_3}},
  note = {Accessed December 2025}
}

% Meta - Llama
@article{llama3,
  title = {The {Llama} 3 Herd of Models},
  author = {{Llama Team, AI @ Meta}},
  journal = {arXiv preprint arXiv:2407.21783},
  year = {2024},
  url = {https://arxiv.org/abs/2407.21783}
}

@misc{llama31,
  title = {{Llama} 3.1 Model Card},
  author = {{Meta}},
  year = {2024},
  howpublished = {\url{https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md}},
  note = {Model card. Accessed December 2025}
}

@misc{llama32,
  title = {{Llama} 3.2 Model Card},
  author = {{Meta}},
  year = {2024},
  howpublished = {\url{https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md}},
  note = {Model card. Accessed December 2025}
}

@misc{llama4,
  title = {{Llama} 4 Model Card},
  author = {{Meta}},
  year = {2025},
  howpublished = {\url{https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md}},
  note = {GitHub model card. Accessed December 2025}
}

% Microsoft - Phi
@article{phi4,
  title = {Phi-4 Technical Report},
  author = {Abdin, Marah and Aneja, Jyoti and Behl, Harkirat and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Harrison, Michael and Hewett, Russell and Javaheripi, Mojan and Kauffmann, Piero and others},
  journal = {arXiv preprint arXiv:2412.08905},
  year = {2024},
  url = {https://arxiv.org/abs/2412.08905}
}

% MiniMax
@misc{minimax_m2,
  title = {{MiniMax-M2}: A Model Built for Max Coding and Agentic Workflows},
  author = {{MiniMax}},
  year = {2025},
  howpublished = {\url{https://huggingface.co/MiniMaxAI/MiniMax-M2}},
  note = {Hugging Face model card. Accessed December 2025}
}

% Mistral AI
@misc{mistral,
  title = {Mistral {AI} Models},
  author = {{Mistral AI}},
  year = {2025},
  howpublished = {\url{https://docs.mistral.ai/getting-started/models/}},
  note = {Model documentation. Accessed December 2025}
}

% Moonshot AI - Kimi
@article{kimi_k2,
  title = {{Kimi K2}: Open Agentic Intelligence},
  author = {{Moonshot AI}},
  journal = {arXiv preprint arXiv:2507.20534},
  year = {2025},
  url = {https://arxiv.org/abs/2507.20534}
}

% Nvidia - Nemotron
@misc{nemotron,
  title = {{Nemotron}: Foundation Models for Agentic {AI}},
  author = {{NVIDIA}},
  year = {2025},
  howpublished = {\url{https://developer.nvidia.com/nemotron}},
  note = {Accessed December 2025}
}

% OpenAI - GPT series
@article{gpt4,
  title = {{GPT-4} Technical Report},
  author = {{OpenAI}},
  journal = {arXiv preprint arXiv:2303.08774},
  year = {2023},
  url = {https://arxiv.org/abs/2303.08774}
}

@article{gpt4o,
  title = {{GPT-4o} System Card},
  author = {{OpenAI}},
  journal = {arXiv preprint arXiv:2410.21276},
  year = {2024},
  url = {https://arxiv.org/abs/2410.21276}
}

@misc{gpt41,
  title = {Introducing {GPT-4.1} in the {API}},
  author = {{OpenAI}},
  year = {2025},
  howpublished = {\url{https://openai.com/index/gpt-4-1/}},
  note = {Accessed December 2025}
}

@misc{gpt5,
  title = {{GPT-5} System Card},
  author = {{OpenAI}},
  year = {2025},
  howpublished = {\url{https://cdn.openai.com/gpt-5-system-card.pdf}},
  note = {System card. Accessed December 2025}
}

@misc{gpt52,
  title = {{GPT-5.2} System Card},
  author = {{OpenAI}},
  year = {2025},
  howpublished = {\url{https://cdn.openai.com/pdf/3a4153c8-c748-4b71-8e31-aecbde944f8d/oai_5_2_system-card.pdf}},
  note = {System card. Accessed January 2026}
}

@misc{o3_o4mini,
  title = {{OpenAI} o3 and o4-mini System Card},
  author = {{OpenAI}},
  year = {2025},
  howpublished = {\url{https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf}},
  note = {System card. Accessed December 2025}
}

% xAI - Grok
@misc{grok3,
  title = {Grok 3 Model Page},
  author = {{OpenRouter}},
  year = {2025},
  howpublished = {\url{https://openrouter.ai/x-ai/grok-3}},
  note = {Model page. Accessed December 2025}
}

@misc{grok,
  title = {Grok 4 Model Card},
  author = {{xAI}},
  year = {2025},
  howpublished = {\url{https://data.x.ai/2025-08-20-grok-4-model-card.pdf}},
  note = {Model card. Accessed December 2025}
}

% Zhipu AI - GLM
@article{glm4,
  title = {{ChatGLM}: A Family of Large Language Models from {GLM-130B} to {GLM-4} All Tools},
  author = {{GLM Team, Zhipu AI}},
  journal = {arXiv preprint arXiv:2406.12793},
  year = {2024},
  url = {https://arxiv.org/abs/2406.12793}
}

@misc{glm47,
  title = {{GLM-4.7} Model Card},
  author = {{Zhipu AI}},
  year = {2025},
  howpublished = {\url{https://huggingface.co/zai-org/GLM-4.7}},
  note = {Hugging Face model card. Accessed December 2025}
}
